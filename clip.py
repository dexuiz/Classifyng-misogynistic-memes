# -*- coding: utf-8 -*-
"""Copy of clip

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xXlEZZh-afbcR_77-atr1soRlJ8fpLK-
"""

import numpy as np
from tqdm.auto import tqdm
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.utils.data import DataLoader,Dataset,IterableDataset,RandomSampler,Subset
import csv
from itertools import cycle
import pandas as pd
import torchvision
from torchvision import datasets
from torchvision.datasets.mnist import MNIST
from torchvision import datasets
import matplotlib.pyplot as plt
import sklearn
from sklearn import metrics
from PIL import Image
from torchvision import utils
import zipfile
import shutil 
import logging
from pathlib import Path
import warnings
import random 
import json
from sklearn.model_selection import train_test_split, KFold,StratifiedKFold

torch.manual_seed(42)
np.random.seed(42)

PATH = '/content/drive/MyDrive/computationSS'

from google.colab import drive
drive.mount('/content/drive')

pd.read_csv('/content/drive/MyDrive/computationSS/training.csv', sep='\t', header=None)

csv_path = os.path.join(PATH,'training.csv')
data_dir = os.path.join(PATH,'TRAINING')

# for binary classification, all fields aren't necessary
fields = ["file_name", "misogynous", "Text Transcription"]
train_samples_frame = pd.read_csv(csv_path, sep='\t', usecols=fields)
train_samples_frame

train_samples_frame['misogynous'].value_counts()

train_samples_frame["Text Transcription"].map(
    lambda text: len(text.split(" "))
).describe()



from PIL import Image


images = [
    Image.open(
        "/".join((data_dir, train_samples_frame.loc[i, "file_name"]))
    ).convert("RGB")
    for i in range(5)
]

for image in images:
    print(image.size)

"""# Samples"""

# define a callable image_transform with Compose
image_transform = torchvision.transforms.Compose(
    [
        torchvision.transforms.Resize(size=(224, 224)),
        torchvision.transforms.ToTensor()
    ]
)

# convert the images and prepare for visualization.
tensor_img = torch.stack(
    [image_transform(image) for image in images]
)
grid = torchvision.utils.make_grid(tensor_img)

# plot
plt.rcParams["figure.figsize"] = (20, 5)
plt.axis('off')
_ = plt.imshow(grid.permute(1, 2, 0))

!mkdir /content/drive/MyDrive/computationSS/data
save_path = "/content/drive/MyDrive/computationSS/data"

# train_df, test_df = train_test_split(train_samples_frame, test_size=0.2, random_state=101, stratify=train_samples_frame["misogynous"])
train_df, val_df = train_test_split(train_samples_frame, test_size=0.1, random_state=101, stratify=train_samples_frame["misogynous"])

train_df = train_df.reset_index(drop=True)
# test_df = test_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)

# saving the data as csv, so we don't need to load all audio files into memory
train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
# test_df.to_csv(f"{save_path}/test.csv", sep="\t", encoding="utf-8", index=False)
val_df.to_csv(f"{save_path}/val.csv", sep="\t", encoding="utf-8", index=False)

print(train_df.shape)
print(val_df.shape)
# print(test_df.shape)
print(train_df['misogynous'].value_counts())
print(val_df['misogynous'].value_counts())
# print(test_df['misogynous'].value_counts())

"""Dataset"""

from pandas_path import path
from transformers import CLIPFeatureExtractor, CLIPTokenizerFast, CLIPModel

class HatefulMemesDataset(torch.utils.data.Dataset):
    """Uses csv data to preprocess and serve 
    dictionary of multimodal tensors for model input.
    """

    def __init__(self, data_path, img_dir, random_state=0,):

        fields = ["file_name", "misogynous", "Text Transcription"]
        self.samples_frame = pd.read_csv(data_path, sep='\t', usecols=fields)
        self.samples_frame = self.samples_frame.reset_index(drop=True)
        self.samples_frame.file_name = self.samples_frame.apply(lambda row: "/".join((img_dir, row.file_name)), axis=1)

        # print(self.samples_frame.file_name.path.exists().where(lambda i: i!=True).dropna())
        # https://github.com/drivendataorg/pandas-path
        if not self.samples_frame.file_name.path.exists().all():
            raise FileNotFoundError
        if not self.samples_frame.file_name.path.is_file().all():
            raise TypeError
            
        self.image_transform = CLIPFeatureExtractor.from_pretrained("openai/clip-vit-base-patch32")
        self.text_transform = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")
        self.tokenized_texts = self.text_transform(self.samples_frame['Text Transcription'].tolist(), truncation=True)

    def __len__(self):
        """This method is called when you do len(instance) 
        for an instance of this class.
        """
        return len(self.samples_frame)

    def __getitem__(self, idx):
        """This method is called when you do instance[key] 
        for an instance of this class.
        """
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_id = pd.Series(self.samples_frame.loc[idx, "file_name"]).apply(lambda i: int(i.split('/')[-1].split('.')[0])).values

        image = Image.open(
            self.samples_frame.loc[idx, "file_name"]
        ).convert("RGB")
        pixel_values = self.image_transform(image, do_resize=True, size=224)['pixel_values'][0]

        input_ids = torch.tensor(self.tokenized_texts['input_ids'][idx])

        
        if "misogynous" in self.samples_frame.columns:
            label = torch.Tensor(
                [self.samples_frame.loc[idx, "misogynous"]]
            ).long().squeeze()
            sample = {
                'input_ids': input_ids,
                'pixel_values': pixel_values,
                "id": img_id,
                "label": label
            }
        else:
            sample = {
                'input_ids': input_ids,
                'pixel_values': pixel_values,
                "id": img_id,
            }

        return sample

"""MODEL using CLIP"""

class LanguageAndVisionConcat(torch.nn.Module):
    def __init__(self, num_classes, loss_fn, clip_model, fusion_output_size, dropout_p):
        super(LanguageAndVisionConcat, self).__init__()
        self.clip_model = clip_model
        self.fusion = torch.nn.Linear(
            in_features=(512 + 512), # CLIP's text embedding dim. + image embedding dim. = 512 + 512
            out_features=fusion_output_size
        )
        self.fc = torch.nn.Linear(
            in_features=fusion_output_size, 
            out_features=num_classes
        )
        self.loss_fn = loss_fn
        self.dropout = torch.nn.Dropout(dropout_p)
        
    def forward(self, input_ids, pixel_values, label=None,attention_mask=None):
        # print(f"TEXT {input_ids.shape}  IMAGE {pixel_values.shape}")
        with torch.inference_mode():
            outputs = self.clip_model(input_ids=input_ids, pixel_values=pixel_values,attention_mask=attention_mask)
          # logits_per_image = outputs.logits_per_image # this is the image-text similarity score

        text_features = torch.nn.functional.relu(outputs['text_embeds'])
        image_features = torch.nn.functional.relu(outputs['image_embeds'])

        combined = torch.cat(
            [text_features, image_features], dim=1
        )
        fused = self.dropout(
            torch.nn.functional.relu(
            self.fusion(combined)
            )
        )
        logits = self.fc(fused)
        pred = torch.nn.functional.softmax(logits)
        loss = (
            self.loss_fn(pred, label) 
            if label is not None else label
        )
        return (pred, loss)

"""Creating a pytorch lightining module for this model"""

import pytorch_lightning as pl
from transformers import DataCollatorWithPadding

# for the purposes of this post, we'll filter
# much of the lovely logging info from our LightningModule
# warnings.filterwarnings("ignore")
# logging.getLogger().setLevel(logging.WARNING)


class HatefulMemesModel(pl.LightningModule):
    def __init__(self, hparams):
        for data_key in ["train_path", "dev_path", "img_dir",]:
            # ok, there's one for-loop but it doesn't count
            if data_key not in hparams.keys():
                raise KeyError(
                    f"{data_key} is a required hparam in this model"
                )
        self.save_hyperparameters()
        super().__init__()
        self.hparams.update(hparams)
        
        
        self.output_path = Path(
            self.hparams.get("output_path", "model-outputs")
        )
        self.output_path.mkdir(exist_ok=True)
        
        
        self.train_dataset = self._build_dataset(self.hparams["train_path"])
        self.dev_dataset = self._build_dataset(self.hparams["dev_path"])
        
        # set up model and training
        self.model = self._build_model()
        self.trainer_params = self._get_trainer_params()
    
    ## Required LightningModule Methods (when validating) ##
    
    def forward(self, input_ids, pixel_values, label=None):
        return self.model(input_ids, pixel_values, label)

    def training_step(self, batch, batch_nb):
        preds, loss = self.forward(
            input_ids=batch["input_ids"],
            pixel_values=batch["pixel_values"],
            label=batch["labels"]
        )
        
        return {"loss": loss}

    def validation_step(self, batch, batch_nb):
        preds, loss = self.eval().forward(
            input_ids=batch["input_ids"],
            pixel_values=batch["pixel_values"],
            label=batch["labels"]
        )
        
        return {"batch_val_loss": loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack(
            tuple(
                output["batch_val_loss"] 
                for output in outputs
            )
        ).mean()
        
        self.log("avg_val_loss",avg_loss)
        return {
            "val_loss": avg_loss,
            #edit
            "avg_val_loss":avg_loss,
            "progress_bar":{"avg_val_loss": avg_loss}
        }

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.model.parameters())
        
        # monitor = [self.hparams.get("early_stop_monitor", "avg_val_loss")],
        return {
          "optimizer": optimizer}
          
    
    def train_dataloader(self):
        data_collator = DataCollatorWithPadding(tokenizer=self.train_dataset.text_transform)

        return torch.utils.data.DataLoader(
            self.train_dataset, 
            shuffle=True, 
            batch_size=self.hparams.get("batch_size", 4), 
            num_workers=self.hparams.get("num_workers", 16),
            collate_fn=data_collator,
        )

    def val_dataloader(self):
        data_collator = DataCollatorWithPadding(tokenizer=self.dev_dataset.text_transform)
        return torch.utils.data.DataLoader(
            self.dev_dataset, 
            shuffle=False, 
            batch_size=self.hparams.get("batch_size", 4), 
            num_workers=self.hparams.get("num_workers", 16),
            collate_fn=data_collator,
        )
    
    ## Convenience Methods ##
    
    def fit(self):
        self._set_seed(self.hparams.get("random_state", 42))
        self.trainer = pl.Trainer(**self.trainer_params)
        self.trainer.fit(self)
        
    def _set_seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

    
  

    def _build_dataset(self, dataset_key):
        return HatefulMemesDataset(
            data_path=dataset_key,
            img_dir=self.hparams.get("img_dir")
        )
    
    def _build_model(self):
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
        #clip_model.eval()

        # vision_module.fc = torch.nn.Linear(
        #         in_features=2048,
        #         out_features=self.vision_feature_dim
        # )

        return LanguageAndVisionConcat(
            num_classes=self.hparams.get("num_classes", 2),
            loss_fn=torch.nn.CrossEntropyLoss(),
            clip_model=clip_model,
            fusion_output_size=self.hparams.get(
                "fusion_output_size", 512
            ),
            dropout_p=self.hparams.get("dropout_p", 0.1),
        )
    
    def _get_trainer_params(self):
        checkpoint_callback = pl.callbacks.ModelCheckpoint(
            dirpath=self.output_path,
            save_last = True,
            monitor=self.hparams.get(
                "checkpoint_monitor", "avg_val_loss"
            ),
            mode=self.hparams.get(
                "checkpoint_monitor_mode", "min"
            ),
            verbose=self.hparams.get("verbose", True)
        )

        early_stop_callback = pl.callbacks.EarlyStopping(
            monitor=self.hparams.get(
                "early_stop_monitor", "avg_val_loss"
            ),
            min_delta=self.hparams.get(
                "early_stop_min_delta", 0.001
            ),
            patience=self.hparams.get(
                "early_stop_patience", 3
            ),
            verbose=self.hparams.get("verbose", True),
        )

        trainer_params = {
            #"callbacks": [checkpoint_callback, early_stop_callback],
            "callbacks": [checkpoint_callback],
            "default_root_dir": self.output_path,
            "accumulate_grad_batches": self.hparams.get(
                "accumulate_grad_batches", 1
            ),
            "gpus": self.hparams.get("n_gpu", 1),
            "max_epochs": self.hparams.get("max_epochs", 100),
            "gradient_clip_val": self.hparams.get(
                "gradient_clip_value", 1
            ),
        }
        return trainer_params
            
    @torch.inference_mode()
    def make_submission_frame(self, test_path):
        preds_model,labels = [],[]
        test_dataset = self._build_dataset(test_path)
        submission_frame = pd.DataFrame(
            # index=test_dataset["file_name"].apply(lambda img_name: img_name.split('.')[0]),
            columns=["proba", "label"]
        )
        data_collator = DataCollatorWithPadding(tokenizer=test_dataset.text_transform)
        test_dataloader = torch.utils.data.DataLoader(
            test_dataset, 
            shuffle=False, 
            batch_size=self.hparams.get("batch_size", 4), 
                        collate_fn=data_collator
        )
        for batch in tqdm(test_dataloader, total=len(test_dataloader)):
            del batch['id']
            del batch['attention_mask']
            targets = batch['labels']
            del batch['labels']

            device = "cuda" if torch.cuda.is_available() else "cpu"
            batch = {k:v.to(device) for k,v in batch.items()}
            self.model.to(device)
            preds, _ = self.model.eval()(**batch)
            pred_label =  preds.argmax(dim=1)
            print(torch.max(preds,dim=1))
            preds_model.extend(pred_label.tolist())
            labels.extend(targets)
            print(preds_model)
        #     submission_frame.loc[batch["id"], "proba"] = preds[:, 1]
        #     submission_frame.loc[batch["id"], "label"] = preds.argmax(dim=1)
        # submission_frame.proba = submission_frame.proba.astype(float)
        # submission_frame.label = submission_frame.label.astype(int)
        return preds_model,labels

"""
**K-Fold Validtion and Training**"""

# train_df, val_df = KFold(train_samples_frame, test_size=0.1, random_state=101, stratify=train_samples_frame["misogynous"])
K = 5
kf = StratifiedKFold(n_splits=K,shuffle=True)

KFold_path = "/content/drive/MyDrive/computationSS/Kfold/"
if not os.path.exists(KFold_path):
    os.makedirs(KFold_path)

y = train_samples_frame["misogynous"]

for i,(train,val) in enumerate(kf.split(train_samples_frame,y)):
    print(len(train_df))
    folds_path = os.path.join(KFold_path,f"fold_{i}")
    
    if not os.path.exists(folds_path):
        os.makedirs(folds_path)
    
    train_df = train_samples_frame.iloc[train]
    val_df = train_samples_frame.iloc[val]

    train_df = train_df.reset_index(drop=True)
    val_df = val_df.reset_index(drop=True)

    # saving the data as csv, so we don't need to load all audio files into memory
    train_df.to_csv(f"{folds_path}/train.csv", sep="\t", encoding="utf-8", index=False)
    val_df.to_csv(f"{folds_path}/val.csv", sep="\t", encoding="utf-8", index=False)

    hparams = {
    # Required hparams
    "train_path": f"{folds_path}/train.csv",
    "dev_path": f"{folds_path}/val.csv",
    "img_dir": data_dir,
    
    # Optional hparams
    #"embedding_dim": 150,
    #"language_feature_dim": 300,
    #"vision_feature_dim": 300,
    #"fusion_output_size": 256,
    "output_path": os.path.join(PATH,f"model-outputs-fold_{i}"),
    "dev_limit": None,
    "lr": 0.00001,
    "max_epochs": 10,
    "n_gpu": 1,
    "batch_size": 16,
    # allows us to "simulate" having larger batches 
    "accumulate_grad_batches": 16,
    "num_workers":0
    }

    print(f"TRAINING MODEL for FOLD {i}")
    hateful_memes_model = HatefulMemesModel(hparams=hparams)
    hateful_memes_model.fit()



"""Training the model"""

train_path = os.path.join(PATH,'data/train.csv')
dev_path = os.path.join(PATH,'data/val.csv')
data_dir = os.path.join(PATH,'TRAINING')

hparams = {
    
    # Required hparams
    "train_path": train_path,
    "dev_path": dev_path,
    "img_dir": data_dir,
    
    # Optional hparams
    #"embedding_dim": 150,
    #"language_feature_dim": 300,
    #"vision_feature_dim": 300,
    #"fusion_output_size": 256,
    "output_path": os.path.join(PATH,"model-outputs-dummy"),
    "dev_limit": None,
    "lr": 0.00001,
    "max_epochs": 10,
    "n_gpu": 0,
    "batch_size": 8,
    # allows us to "simulate" having larger batches 
    "accumulate_grad_batches": 16,
}

hateful_memes_model = HatefulMemesModel(hparams=hparams)

# trainer = pl.Trainer(accelerator='gpu', devices=1,max_epochs=10)
# trainer.fit(hateful_memes_model)
hateful_memes_model.fit()

"""KLFold Evaluation"""

#kFold evaluation 
from sklearn.metrics import f1_score,accuracy_score,confusion_matrix,precision_score,recall_score
f1_arr = []
acc_arr = []

for fold in range(5):
    path = f"/content/drive/MyDrive/computationSS/model-outputs-fold_{fold}/last.ckpt"
    model = hateful_memes_model.load_from_checkpoint(path, layers=3, drop_rate=0)
    preds,labels = model.make_submission_frame('/content/drive/MyDrive/computationSS/data/val.csv')
    labz = [i.item() for i in labels]
    f1 = f1_score(labz,preds)
    acc = accuracy_score(labz,preds)
    f1_arr.append(f1)
    acc_arr.append(acc)

np.mean(f1_arr)

np.mean(acc_arr)

trainer = pl.Trainer()
chk_path = "/content/drive/MyDrive/computationSS/model-outputs2/last.ckpt"
model2 = hateful_memes_model.load_from_checkpoint(chk_path, layers=3, drop_rate=0)
# results = trainer.test(model=model2, datamodule=, verbose=True)



preds,labels = model2.make_submission_frame('/content/drive/MyDrive/computationSS/data/val.csv')

labz = [i.item() for i in labels]

from sklearn.metrics import f1_score,accuracy_score,confusion_matrix,precision_score,recall_score
print(f1_score(labz,preds))
print(accuracy_score(labz,preds))
print(precision_score(labz,preds))
print(recall_score(labz,preds))
print(confusion_matrix(labz,preds))



"""INFERENCING entire folders of memes"""

#running alot of inference 
chk_path = "/content/drive/MyDrive/computationSS/model-outputs2/last.ckpt"
load_model = hateful_memes_model.load_from_checkpoint(chk_path, layers=3, drop_rate=0)

'''
we are using CUDA acceleration if available with PyTorch
'''
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using {device} device')

"""Dataloader for a folder"""

class inferenceDataset(Dataset):
    def __init__(self,folder_path):
        self.folder_path = folder_path
        file_list = os.listdir(folder_path)
        file_list = [i.split(".")[0] for i in file_list if "jpg" in i]
        self.file_list = file_list

        self.image_transform = CLIPFeatureExtractor.from_pretrained("openai/clip-vit-base-patch32")
        self.text_transform = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")
        # l = []
        # for i in self.file_list:
        #     js = json.load(open(os.path.join(self.folder_path,i+".json")))
        #     if "ocr" in js:
        #         l.append(i)
        # self.file_list = l
        # print(f"files to be processed {len(self.file_list)}")
        # self.tokenized_texts = self.text_transform(self.samples_frame['Text Transcription'].tolist(), truncation=True)
    def __getitem__(self,idx):
        selected_file = self.file_list[idx]
        selected_path = os.path.join(self.folder_path,selected_file)
        selected_json = json.load(open(selected_path+".json"))
        img_id = selected_json["id"]

        image = Image.open(
            selected_path+".jpg"
        ).convert("RGB")
        text_data =selected_json["title"]
        if "ocr" in selected_json and len(selected_json["ocr"])>1:
            text_data = selected_json["ocr"]
        pixel_values = self.image_transform(image, do_resize=True, size=224)['pixel_values'][0]
        tokenized_ocr = self.text_transform(text_data,truncation=True,padding='max_length',max_length=77)
        input_ids = torch.tensor(tokenized_ocr['input_ids'])
        attention_mask = torch.tensor(tokenized_ocr['attention_mask'])
        sample = {
          'input_ids': input_ids,
          'attention_mask':attention_mask,
          'pixel_values': pixel_values,
          "id": img_id,
      }

        return sample

    def __len__(self):
        return len(self.file_list)

def get_dataloader(dataset_object,batch_size):

    # data_collator = DataCollatorWithPadding(tokenizer=dataset_object.text_transform)

    return torch.utils.data.DataLoader(
        dataset_object, 
        shuffle=False, 
        batch_size=batch_size, 
        num_workers=2,
        # collate_fn=data_collator,
    )

from IPython.core.display import ProgressBar
def inference_on_folder(pl_model,dataloader):
    pl_model.model.to(device)
    pl_model.model.eval()
    model = []
    images = []
    progress = tqdm(range(len(dataloader)))
    for batch in dataloader:
        img_ids = batch['id']
        batch = {k:v.to(device) for k,v in batch.items() if k!="id"}
        # print(batch['pixel_values'].shape)
        outputs = pl_model.model(input_ids=batch['input_ids'],pixel_values=batch['pixel_values'],attention_mask=batch['attention_mask'])
        preds,_ = outputs
        model_res = torch.argmax(preds,dim=1).cpu().tolist()
        max_prob = torch.max(preds,dim=1)
        print("MAX_prob",max_prob)
        model.extend(model_res)
        images.extend(img_ids)
        progress.update(1)
    return model,images

"""Running the inference"""

folder_list = ['meme','memes','Memes_Of_The_Dank','darkmemers','MemeYourEnthusiasm','dankmemes','Grimdank','MemesIRL','dankmeme','okbuddyretard']
# folder_list = ['Memes_Of_The_Dank','darkmemers','MemeYourEnthusiasm','dankmemes','Grimdank','MemesIRL','dankmeme','okbuddyretard']
# folder_list=["terriblefacebookmemes","ComedyCemetery"]

batch_size = 16
PATH_2 = "/content/drive/MyDrive/CSSdata2"
postfix ="~filtered"
for f in :
    folder_path = os.path.join(PATH,f)
    if not os.path.isdir(folder_path):
        continue
    # if len(os.listdir(folder_path))<8:
    #     continue
    print(f"processing folder {f}")
    dataset_folder = inferenceDataset(folder_path)
    # print(dataset_folder[1])
    dataloader = get_dataloader(dataset_folder,batch_size)
    preds,images = inference_on_folder(load_model,dataloader)

    writer = [f"{str(i[0])}\t{str(i[1])}\n" for i in zip(images,preds)]

    output_path = os.path.join(folder_path,"RESULTS.txt")
    with open(output_path,"w") as fi:
      fi.writelines(writer)

