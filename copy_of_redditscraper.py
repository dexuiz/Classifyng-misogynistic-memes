# -*- coding: utf-8 -*-
"""Copy of RedditScraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1riVvXXlYD31ctU_GBvWf1gUvOVxwy1pt
"""



!pip install praw
import praw 
import urllib.request
import os 
from PIL import Image
import json 
from tqdm.auto import tqdm
import requests
from meme_ocr.memeocr import MemeOCR        
import re

"""
Image downloader
"""
def downloader(link,path):
    # f= urllib.request.urlretrieve(link, path)
    # print(f)
    # print(f"downloading {link}")
    # return f
    try:
        img= requests.get(link)
        if img.status_code!=200:
            return
        # print(img)
        img_data = img.content
        with open(path, 'wb') as handler:
            handler.write(img_data)
        return True
    except:
      print("download failed")
      return False

"""
PRAW instance to scrape reddit
"""
reddit = praw.Reddit(
    client_id="dQG4qfGiEo-KjQV8wd11jQ",
    client_secret="5oekxFnVLknPdwSwIHTWBSdkbzD45Q",
    user_agent="my user agent",
    check_for_async=False
)

"""SUBREDDIT LIST"""
subreddit_list = ['meme','memes','Memes_Of_The_Dank','darkmemers','MemeYourEnthusiasm','dankmemes','Grimdank','MemesIRL','dankmeme','okbuddyretard',"terriblefacebookmemes","ComedyCemetery"]

PATH  = "/content/drive/MyDrive/CSSdata2"
if not os.path.exists(PATH):
    os.makedirs(PATH)

import pandas as pd
import requests
import json
import time
"""
Function to read data using Pushshift
"""
def getPushshiftData(after, sub,searchq=None):
    if searchq!=None:
        url = 'https://api.pushshift.io/reddit/search/submission?&size=500&after='+str(after)+'&subreddit='+str(sub)+'&q='+str(searchq)
    else:
        url = 'https://api.pushshift.io/reddit/search/submission?&size=500&after='+str(after)+'&subreddit='+str(sub)
    time.sleep(0.7)
    r = requests.get(url)
    if len(r.text)>5 and r.status_code==200:
        data = json.loads(r.text)
        return data['data']
    else:
        print("failed",r.status_code,r.text)
        return []

import pandas as pd
import requests
import json
import time

def getPushshiftData(after, sub,searchq=None):
    if searchq!=None:
        url = 'https://api.pushshift.io/reddit/search/submission?&size=500&after='+str(after)+'&subreddit='+str(sub)+'&q='+str(searchq)
    else:
        url = 'https://api.pushshift.io/reddit/search/submission?&size=1000&after='+str(after)+'&subreddit='+str(sub)
    time.sleep(0.5)
    r = requests.get(url)
    if len(r.text)>5 and r.status_code==200:
        data = json.loads(r.text)
        return data['data']
    else:
        return []
"""
Downloading all jsons
"""
for sub in subreddit_list:
    #list of post ID's
    posts = []
 
    print(f"processing subreddit {sub}")
    after = "1647789753"
    sixmonths = '1637520835'
    ayear = "1619031835"
    twoyear = "1588003650"
    searchq='women | woman '
    data = getPushshiftData(twoyear, sub,searchq=searchq)
    print(data)
    # Will run until all posts have been gathered 
    # from the 'after' date up until todays date
    while len(data) > 0 and len(posts)<25000:
        for submission in data:
            posts.append(submission)
        # Calls getPushshiftData() with the created date of the last submission
        print(f"Scraped {len(posts)}")
        data = getPushshiftData(sub=sub, after=data[-1]['created_utc'],searchq=searchq)


    obj = {}
    # obj['sub'] = sub
    obj[sub] = posts



    # Save to json for later use
    with open(f"{PATH}/{sub}~{searchq}.json", "w") as jsonFile:
        json.dump(obj, jsonFile)

"""
Downloading all images for jsons
"""
for jfile in os.listdir(os.path.join(PATH,'sub_files_filtered')):
    json_path = os.path.join(PATH,"sub_files_filtered",jfile)
    all_posts = json.load(open(json_path,'r'))
    sub_name = jfile.split("~")[0]
    print(f"processing subreddit {sub_name}")
    save_path = os.path.join(PATH,sub_name+"~filtered")
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    progress_bar = tqdm(range(len(all_posts[sub_name])))
    for submission in all_posts[sub_name]:
        data = {}
        if submission['is_video'] == True or submission['is_self']==True:
            continue
        if 'post_hint' in submission and 'video' in submission['post_hint']:
            continue

        try:
            link = submission['url']
            # print(link)
            # print(data['link'])
            if 'jpg' in link:
                file_name = os.path.join(save_path,submission['id'])
            else:
                continue
            stat = downloader(link,file_name+".jpg")
            if stat==False:
                continue
            if not os.path.exists(file_name+".jpg"):
                continue
            demo = Image.open(file_name+".jpg").convert("RGB")
        except KeyboardInterrupt:
            continue

        with open(file_name+".json","w") as f:
             json.dump(submission,f)
        progress_bar.update(1) 
    #     break

"""##add ocr text to images"""
ocr = MemeOCR()
postfix = "~filtered"
for subreddit in  ["terriblefacebookmemes","ComedyCemetery"]:
    folder_path = os.path.join(PATH,subreddit+postfix)
    if not os.path.isdir(folder_path):
        continue
    total_files = os.listdir(folder_path)
    total_files = [i for i in total_files if "jpg" in i]
    print(f"processing subreddit {subreddit}")
    progress_bar = tqdm(range(len(total_files)))
    c = 0
    for f in total_files:
        meme_path = os.path.join(folder_path,f)
        meme_json_path = meme_path.split(".")[0]+".json"
        if not os.path.exists(meme_json_path):
            continue
        meme_json = json.load(open(meme_json_path,"r"))
        text = ocr.recognize(meme_path)
        text = " ".join(text)
        text  = re.sub(r'[^A-Za-z0-9 ]+', '', text)
        if len(text)>0:
            meme_json["ocr"] = text
        else:
            meme_json["ocr"] = ""
        with open(meme_json_path,"w") as f:
            json.dump(meme_json,f)
        
        progress_bar.update(1)
        c+=1
        # if c>1000:
        #     break

"""collect upvote information from posts"""
for sub in subreddit_list:
    #list of post ID's
    posts = []
    #Subreddit to query
    # sub='okbuddyretard'
    # Unix timestamp of date to crawl from.
    # 2022/03/20
    print(f"processing subreddit {sub}")
    after = "1647789753"
    sixmonths = '1637520835'
    ayear = "1619031835"
    twoyear = "1588003650"
    data = getPushshiftData(twoyear, sub)
    try:
      current = int(ayear)
      while len(data)>0:
          for submission in data:
              id = submission["id"]
              submission["upvotes"] = reddit.submission(id=id).score
              posts.append(submission)
          current += 2592000
          print(f"Scraped {len(posts)}")
          data = getPushshiftData(sub=sub, after=current)
    except:
      pass
      
    obj = {}
    # obj['sub'] = sub
    obj[sub] = posts



    # Save to json for later use
    with open(f"{PATH}/{sub}~overall.json", "w") as jsonFile:
        json.dump(obj, jsonFile)
#didnt process meme your enthusisasm

